# Homework 2
  
## Part 1: Concept questions

```{r eval=T, message=F}
#import libraries
library(vegan)
library(survival)
library(bnlearn)
library(dplyr)
library(ggplot2)
library(lattice)
library(mice)
library(ROCR)
library(tableone)
library(caret)
library(rpart)
library(e1071)
```

```{r eval=T, message=T}
# synthetic depression data
depressionData = data.frame( # do not change "depressionData"
  pregnant = c(1,0,1,1),
  depressed = c("yes","yes","no","no") %>% as.factor(),
  hospitalized = c(1, 0, 0, 0) %>% as.logical()
) %>% tbl_df()

# tree: a model that outputs the odds of hospitalization from inputs of data (datums)
tree = data.frame( # do not change "tree"
  splitVariable = c("depressed", "pregnant", NA, NA, NA),
  split = c("yes", 1, NA, NA, NA),
  trueChild = c(2, 4, NA, NA, NA),
  falseChild = c(3, 5, NA, NA, NA),
  odds = c(NA, NA, 0.1, 2, 3)
)

predictOddsOnDataSet = function(tree, data, active = 1) {
  apply(data, 1, (function(x) {predictedOdds(tree=tree, x, active=1)}) )
}

predictedOdds = function(tree, datum, active = 1) {
  if(is.na(tree[active,"splitVariable"])) { # leaf of tree, so output value 
    return(tree$odds[active])
  } else { # internal node of tree, so continue down tr ee to true/false child
    if( (datum[[tree[active,"splitVariable"] %>% as.character]] %>% as.character) == tree[active,"split"])
      return(predictedOdds(tree, datum, active = tree[active,"trueChild"])) else
        return(predictedOdds(tree, datum, active = tree[active,"falseChild"])) }
}
```
  
First, verify to yourself that, for the fourth patient in ```depressionData```, the tree should have output an odds of 0.1.
```{r eval=T, message=T}
predictedOdds(tree, depressionData[3,], active=1)
```
Fix the function ```predictedOdds``` so that ```predictedOddsOnDataSet``` outputs the odds for each patient in data. Use the debugger functions like ```debugOnce(predictedOdds)``` or ```browser()``` to inspect the code. What did you change?

*The recursive call of the  ```predictedOdds() ``` function had a syntax error in the last argument:
To either get the false or true child of the current node, one has indicate the value in the corresponding row by a string. I changed  ```tree[active,falseChild] ``` to  ```tree[active,'falseChild']``` and  ```tree[active,trueChild] ``` to  ```tree[active,'trueChild'] ```.*

Add a column of the predicted probabilities of hospitalization to depressionData. Display it.
```{r eval=T, message=T}
depressionData <- depressionData %>% mutate(probabilityHospitalized = predictOddsOnDataSet(tree,depressionData))
depressionData
```

Using a threshold probability of 0.5, what is:
```{r eval=T, message=F}
depressionData <- mutate(depressionData, predictedHospitalized = ifelse(probabilityHospitalized > 0.5,"TRUE","FALSE"))
TP <- sum(depressionData$hospitalized=="TRUE" & depressionData$predictedHospitalized=="TRUE")
FP <- sum(depressionData$hospitalized=="FALSE" & depressionData$predictedHospitalized=="TRUE")
FN <- sum(depressionData$hospitalized=="TRUE" & depressionData$predictedHospitalized=="FALSE")
TN <- sum(depressionData$hospitalized=="FALSE" & depressionData$predictedHospitalized=="FALSE")
```

- the accuracy of the model?
```{r eval=T, message=T}
accuracy <- sum(depressionData$hospitalized==depressionData$predictedHospitalized) / nrow(depressionData) 
#or
accuracy <- (TP+TN) / (TP+FN+FP+TN) 
accuracy
```

- the sensitivity of the model?
```{r eval=T, message=t}
sensitivity <- TP/(TP+FN)
sensitivity
```

- the specificity of the model?
```{r eval=T, message=T}
specificity <- 1-(FP/(TN+FP))
specificity
```

- the precision of the model?
```{r eval=T, message=T}
precision <- TP/(TP+FP)
precision
```

- the recall of the model?
```{r eval=T, message=T}
recall <- TP/(TP+FN)
recall
```


Suppose you want to know the prevalence of diabetes in Pittsburgh. If you randomly survey 10 Pittsburghers and 5 of them state they have diabetes:

- what is the maximum likelihood estimate for the prevalence of diabetes?
```{r eval=T, message=T}
individualsWithDiabetes <- 5
individualsWithoutDiabetes <- 5
MLE <- individualsWithDiabetes/
  (individualsWithDiabetes+individualsWithoutDiabetes)
MLE
```

- given your strong belief specified by a beta prior of $\alpha = 11, \beta = 21$, what is the maximum a posteriori estimate for the prevalence of diabetes?
```{r eval=T, message=T} 
alpha <- 11
beta <- 21
MAP <- (individualsWithDiabetes + alpha -1) /
  (individualsWithDiabetes+individualsWithoutDiabetes+alpha+beta-2)
MAP
```

## Part 2: Analysis (9 points)

#### Preliminaries
- **Y:** What was the definition of the primary outcome in this study?
- What is (are) the variable name(s) for the outcome?

*The two primary outcomes were  'death within 14 days' and 'death, dependency, and incomplete recovery' at 6 months. Meanwhule the protocol just specified the latter one as 'death or dependency (ie, needing help from another person with daily activities) at 6 months'.*
*For the first study outcome after 14 days, the 'DDEAD' variables indicates wehther a patient was 'Dead on discharge form'.*
*For the study outcome after six month, 'FDEAD' indicates if the patient was 'Dead at six month follow-up'. The variable 'FDENNIS' states whether the patient was 'Dependent at six month follow-up' and 'FRECOVER' finally describes if the individual was 'Fully recovered at six month follow-up'. All of these variables are set to either false ('F') or true ('T').*
*The variable 'OCCODE' furthermore describes the study outcome after six month in more granular way by providing five potential values: 1:dead, 2:dependent, 3:not recovered, 4:recovered, 8 or 9: missing status.*

- **U:** what is (are) the variable name(s) for the intervention, and what is (are) their possible values?

*The two main variables that are describing the intervention are the following:*
*'RXASP', which indicates whther aspirin was allocated by either stating true ('Y') or false ('N') and*
*'RXHEP' that states if heparin was allocated based on four levels: high (H), medium (M), low(L) and none(N)*

*Besides these two variables, the following columns highlight several non-trial treatments that were undertaken in the hospital:*

*'DASP14': Aspirin given for 14 days or till death or discharge (Y/N) *

*'DLH14': Low dose heparin given for 14 days or till death/discharge (Y/N) *

*'DMH14': Medium dose heparin given for 14 days or till death/discharge (Y/N) *

*'DHH14': Medium dose heparin given for 14 days etc in pilot (combine with above) *

*'DOAC': Other anticoagulants (Y/N) *

*'DGORM': Glycerol or manitol (Y/N)*

*'DSTER': Steroids (Y/N)*

*'DCAA': Calcium antagonists (Y/N)*

*'DHAEMD': Haemodilution (Y/N)*

*'DCAREND': Carotid surgery (Y/N)*

*'DTHROMB': Thrombolysis (Y/N)*

*'DMAJNCH': Major non-cerebral haemorrhage (Y/N)*


- **V, W:** describe the covariates included and the population being studied.

*The covariats of the study that were measured during the initial visit of the individual at the hospital include the following:*
*Each hospital was assigned a number within the column 'HOSPNUM'. 'RDELAY' states the delay between the onset of the stroke and the randomisation. The conscious level of the individual was observed ('RCONSC'). Gender ('SEX') and age ('AGE') was stored as well as the syntoms noted on waking ('RSLEEP'). Atrial fibrillation ('RATRIAL'), the potential use of CT before randomisation ('RCT') and the visibility of the infarct on the CT ('RVISINF') was captured. The intake of both study drugs aspirin and heparin 48 hours before randomising was documented as 'RHEP24' and 'RASP3'. 'RSBP' indicates Systolic blood pressure at randomisation. Then the varibales from 'RDEF1' to 'RDEF8' identify different deficit at the body of the individual. Additionally, several time measurements state at what time the randomisation was realized('RDATE', 'HOURLOCAL', 'MINLOCAL', 'DAYLOCAL'). Finally, several further attributes describe the 14 day study period at the hospital, some additional indicators and computed probabilities.*


*A patient was eligible for the study, if a physician identified an acute stroke, where the severity was not taken into account, but the onset must have been less than 48 hours previously. Further, there should be no evidence about intracranial haemorrhage and no clear indications for heparin or aspirin and neither contraindications to heparin or aspirin.In total, 19 435 patients were observed, which were between 16 and 99 years old with an average of 71.72 years. 46% of the population was female and 54% male. The delay of the stroke and the randomization was between one and 48 hours with a mean of 20.12 hours. 71% of the patients were awake during the onset, the other 29% were sleeping. The conscious level of the patients was mostly alert (77%), 22% were classified as drowsy and 1% was without consciousness. *



- Construct a so-called Table 1 for groups of {aspirin, no aspirin} use, including information on age, gender, systolic blood pressure, and conscious state.
```{r eval=T, message=T}
# Get dataset from: http://datashare.is.ed.ac.uk/bitstream/handle/10283/128/IST_corrected.csv

data <- read.csv("IST_corrected.csv")
listVariables <- c("AGE", "SEX", "RSBP", "RCONSC")
categorialVariables <- c("SEX","RCONSC")
table1 <- CreateTableOne(vars = listVariables, data = data, 
              factorVars = categorialVariables,  strata = "RXASP")
table1
```


#### Machine learning analysis
Note: for this analysis, use a simple 50-50 train-test split.
```{r eval=T, message=F}
dataTrain = data[seq(1,nrow(data),2),]
dataTest = data[seq(2,nrow(data),2),]
```

Let our outcome of interest be "dead or dependent at 6 months", i.e. so that we have a binary classification problem.
```{r eval=T, message=F}
dataTrain <- mutate(dataTrain,DOD=ifelse(OCCODE==1|OCCODE==2,1,0))
dataTest <- mutate(dataTest,DOD=ifelse(OCCODE==1|OCCODE==2,1,0))
```
What percent of patients are dead or dependent at 6 months in your train set and test set?
```{r eval=T, message=T}
percentDeadInTrain <- sum(dataTrain$OCCODE==1 | dataTrain$OCCODE==2)/nrow(dataTrain)
#alternatively: percentDeadInTrain <- sum(dataTrain$FDEAD=="Y" | dataTrain$FDENNIS=="Y")/nrow(dataTrain)
paste("Dead or dependent at 6 months in your train set: ",toString(percentDeadInTrain), "%")

percentDeadInTest <- sum(dataTest$OCCODE==1 | dataTest$OCCODE==2)/nrow(dataTest)
#alternatively: percentDeadInTest <- sum(dataTest$FDEAD=="Y" | dataTrain$FDENNIS=="Y")/nrow(dataTest)
paste("Dead or dependent at 6 months in your test set: ", toString(percentDeadInTest), "%")

```

Choose which variables to include in your model. For example, remove variables for outcomes at 14 days (because if you are dead at 14 days you are certainly dead at 6 months). Moreover, you should remove all features measured after baseline if you want to make a prediction based on baseline data. Similarly, specific indicators of the outcome should also be removed, since those are measurements past the baseline that are not our outcome of interest. For these reasons, you will need to remove clusters of variables. Justify your approach.

*First, just the variables from the section "Randomisation data" (according to: http://datashare.is.ed.ac.uk/bitstream/handle/10283/128/IST_variables.pdf) where included. This excludes all the information that were not collected before Randomisation at the beginning of the study. Afterwards, only the country code ("CNTRYNUM") is added as well as 'OCCODE' and 'DNOSTRK' columns, which are used for cleaning and described later. Finally, the dummy variable "DOD" is kept for training and testing data as a label.*
*All other excluded varibles basically contained study results after 14 days or indicated or diagnosed variables, which was not part of the scope.*
```{r eval=T, message=F}
dataTrain <- dataTrain[c(which( colnames(dataTrain)=="HOSPNUM") : which( colnames(dataTrain)=="RXHEP"), which( colnames(dataTrain)=="CNTRYNUM"), which( colnames(dataTrain)=="DOD"), which( colnames(dataTest)=="OCCODE"), which( colnames(dataTest)=="DNOSTRK"))]
dataTest <- dataTest[c(which( colnames(dataTest)=="HOSPNUM") : which( colnames(dataTest)=="RXHEP"), which( colnames(dataTest)=="CNTRYNUM"), which( colnames(dataTest)=="DOD"), which( colnames(dataTest)=="OCCODE"), which( colnames(dataTest)=="DNOSTRK"))]
```

Of the remaining variables, decide whether to exclude variables with missing data, impute them, and/or use indicator variables. (Note that if you choose multiple imputation for some variables, you would need to pool the results when evaluating performance, however for homework you may just use the first imputed data set). Justify your approach.

*To remove individuals with unknown outcome after six month, the 'OCCODE' variable is analyzed and rows removed, where no outcome of the study was not documented*
```{r eval=T, message=F}
dataTrain <- dataTrain[dataTrain$OCCODE<8,]
dataTest <- dataTest[dataTest$OCCODE<8,]
dataTrain <- dataTrain[-c(which( colnames(dataTrain)=="OCCODE"))]
dataTest <- dataTest[-c(which( colnames(dataTest)=="OCCODE"))]
```

*Furthermore, specific cases were discovered, where individuals' initial event was not a stroke. The variable 'DNOSTRK' identifies these individuals with the value 'Y'. Since these rows might cause noise within the training and test data set, the corresponding instances were removed, too.*
```{r eval=T, message=F}
dataTrain <- dataTrain[dataTrain$DNOSTRK=="N",]
dataTest <- dataTest[dataTest$DNOSTRK=="N",]
dataTrain <- dataTrain[-c(which( colnames(dataTest)=="DNOSTRK"))]
dataTest <- dataTest[-c(which( colnames(dataTest)=="DNOSTRK"))]
```

*The provided variables have not been modified further except for changing the dummy variable to a factor. Besides that, the Tree Augmented Naive Bayes requires all variables to be a factor, therefore all non-factor variables were dropped for this algorithm. This approach was chosen due to the fact that factoring variables like the hospital number "HOSPNUM" would create 452 levels, which result in unequal levels in test and training data.*
```{r eval=T, message=T}
# Factor the binary outcome variable
dataTrain$DOD <- as.factor(dataTrain$DOD)
dataTest$DOD <- as.factor(dataTest$DOD)
tanTrain <- dataTrain[ ,sapply(dataTrain, is.factor)]
tanTest <- dataTest[ ,sapply(dataTest, is.factor)]
```


Use the following machine learning algorithms: logistic regression, naive Bayes, Tree Augmented Naive Bayes, and decision tree (specify any parameters you set that are not the default). The packages that you may find useful here are: "glm", "bnlearn", and "rpart", but you may use others if desired. In a table, report the accuracy with 95% confidence intervals for each algorithm.
```{r eval=T, message=T}

############## Naive Bayes   ##############

nb = naiveBayes(DOD~.,dataTrain)
bayesAccuracy <- sum(predict(nb, dataTest)==dataTest$DOD) / nrow(dataTest)
bayesPredictedProbabilities <- predict(nb, dataTest,"raw")
bayesPred <- prediction( bayesPredictedProbabilities[,2], dataTest$DOD)
bayesPerfROC <- performance(bayesPred,"tpr","fpr")
bayesPerfPR <- performance(bayesPred, "prec", "rec")

############## Tree Augmented NB ##############

tan = tree.bayes(tanTrain, "DOD")
fittedTan = bn.fit(tan, tanTrain)
tanAccuracy <- (sum(predict(object=fittedTan, data=tanTest)==tanTest$DOD)) / nrow(tanTest)
tanPredictedProbabilities <- attr(predict(object=fittedTan, data=tanTest, prob=TRUE),"prob")
tanPredictedProbabilities <- data.frame(t(tanPredictedProbabilities))
tanPred <- prediction( tanPredictedProbabilities[2], tanTest$DOD)
tanPerfROC <- performance(tanPred,"tpr","fpr")
tanPerfPR <- performance(tanPred, "prec", "rec")

############## Logistic Regression ##############

lr <- glm(formula = DOD  ~ .,
          family=binomial(link="logit"),data = dataTrain, control = list(maxit = 100))
lrAccuracy <- (sum(predict(lr, dataTest, type="response")>0.5 & dataTest$DOD==1) 
                + sum(predict(lr, dataTest, type="response")<0.5 & dataTest$DOD==0) ) / nrow(dataTest)

lrPredictedProbabilities <- predict(lr, dataTest, type="response")
linPred <- prediction( lrPredictedProbabilities, dataTest$DOD)
linPerfROC <- performance(linPred,"tpr","fpr")
linPerfPR <- performance(linPred, "prec", "rec")

############## Decision Tree ##############

tree <- rpart(DOD~ .,
             data=dataTrain,
             method="class")
dtAccuracy <- (sum(predict(tree, dataTest, type="class")==dataTest$DOD)) / nrow(dataTest)
dtPredictedProbabilities <- predict(tree, dataTest, type = "prob")
dtPred <- prediction( dtPredictedProbabilities[,2], dataTest$DOD)
dtPerfROC <- performance(dtPred,"tpr","fpr")
dtPerfPR <- performance(dtPred, "prec", "rec")

############## Accuracy comparison  ##############
n <- nrow(dataTest)
dtAccuracyInCI <-  paste(toString(dtAccuracy-1.96*sqrt(dtAccuracy*(1-dtAccuracy)/n)),
                         toString(dtAccuracy+1.96*sqrt(dtAccuracy*(1-dtAccuracy)/n)),sep="; ")
lrAccuracyInCI <- paste(toString(lrAccuracy-1.96*sqrt(lrAccuracy*(1-lrAccuracy)/n)),
                        toString(lrAccuracy+1.96*sqrt(lrAccuracy*(1-lrAccuracy)/n)), sep="; ")
tanAccuracyInCI <- paste(toString(tanAccuracy-1.96*sqrt(tanAccuracy*(1-tanAccuracy)/n)),
                         toString(tanAccuracy+1.96*sqrt(tanAccuracy*(1-tanAccuracy)/n)), sep="; ")
bayesAccuracyInCI <- paste(toString(bayesAccuracy-1.96*sqrt(bayesAccuracy*(1-bayesAccuracy)/n)),
                           toString(bayesAccuracy+1.96*sqrt(bayesAccuracy*(1-bayesAccuracy)/n)), sep="; ")
accuracyComparisonMatrix <- matrix(c(dtAccuracyInCI,lrAccuracyInCI,tanAccuracyInCI,bayesAccuracyInCI),ncol=4,byrow=TRUE)
colnames(accuracyComparisonMatrix) <- c("Decision Tree","Logistic Regression", "Tree Augmented Naive Bayes", "Naive Bayes")
rownames(accuracyComparisonMatrix) <- c("Accuracy")
accuracyComparisonTable <- as.table(accuracyComparisonMatrix)
accuracyComparisonTable
```

Construct an ROC (receiver operating characteristic) curve for each model and overlay them on a graph using ggplot. Include a legend.
```{r eval=T, message=T}
plot(linPerfROC, col="blue")
plot(tanPerfROC, add = TRUE, col="red")
plot(bayesPerfROC, add = TRUE, col="green")
plot(dtPerfROC, add = TRUE, col="violet")
title(main="ROC Comparison of different Classifiers")
legend("bottom",c("Logistic Regression","Tree Augmented Naive Bayes","Bayes Network","Decision Tree"),
       col=c("blue","red", "green","violet"), lwd=3, y.intersp = 1.0)
```

Construct a PR (precision recall) curve for each model. Include a legend.
```{r eval=T, message=T}
plot(linPerfPR, col="blue")
title(main="Precision/Recall Curve of Logistic Regression")
legend("bottom",c("Logistic Regression"),col=c("blue"), lwd=5)
plot(tanPerfPR, col="red")
title(main="Precision/Recall Curve of Tree Augmented Naive Bayes")
legend("bottom",c("Tree Augmented Naive Bayes"),col=c("red"), lwd=5)
plot(bayesPerfPR, col="green")
title(main="Precision/Recall Curve of Bayes Network")
legend("bottom",c("Bayes Network"),col=c("green"), lwd=5)
plot(dtPerfPR, col="violet")
title(main="Precision/Recall Curve of Decision Tree")
legend("bottom",c("Decision Tree"),col=c("violet"), lwd=5)
```

#### Conclusions
Let's draw conclusions from this study. Specifically,
- how well are we able to predict death or dependence at 6 months? 

*Due to the low accuracies of the applied algoirthms (all below 0.75), one can state that one could not build a conclusive model. Comparing the different algorithms, identifies the Decision Tree and the Logisitc Regression to perform slightly better the Naive Bayes and Tree Augmented Naive Bayes. However, the error rate of all models outlines the lack of significance. *

- what is the average treatment effect of aspirin on death or dependence at 6 months? - Is aspirin significantly better than the alternative? 
```{r eval=T, message=T}
data <- mutate(data,DOD=ifelse(OCCODE==1|OCCODE==2,1,0))
aspirinATE <- ((sum(data$DOD==1&data$RXASP=="N")/sum(data$RXASP=="N"))-(sum(data$DOD==1&data$RXASP=="Y")/sum(data$RXASP=="Y"))) * 100
paste("Aspirin ATE: ", toString(aspirinATE),"%")
heparinATE <-((sum(data$DOD==1&data$RXHEP=="N")/sum(data$RXHEP=="N"))-(sum(data$DOD==1&data$RXHEP!="N")/sum(data$RXHEP!="N"))) * 100
paste("Heparin ATE: ",toString(heparinATE), "%")
```
*From this small Average Treatment Effect of Aspirin of 1.3% (which is also descibed in the paper) compared to 0.0% of Heparin, one can conclude that Aspirin is not significantally decreasing the outcome of death or dependence at six month.*

- of the algorithms tested, which algorithms perform the best? Justify your statement.

*Considering the results of each classifier on the test set, which where presented above, we can conclude that the Logistic Regression as well as the Decision Tree perform better than the other two classifiers considering a higher accuracy. Additionally, we can use the plotted graphs to analyze the 'True Positive Rate' with the 'False Positive Rate' in the ROC curve. The ROC curve focuses  on the prediction of true outcome values, which is 'dead or dependent' in this case. Comparing the graphs of the different classifiers, shows that the Logistic Regression performs best again, but the Bayes Network is second.  *
